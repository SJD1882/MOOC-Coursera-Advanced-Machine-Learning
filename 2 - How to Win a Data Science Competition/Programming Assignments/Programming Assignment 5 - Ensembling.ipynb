{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming assignment 5: Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.13.1\n",
      "pandas 0.20.3\n",
      "scipy 0.19.1\n",
      "sklearn 0.19.0\n",
      "lightgbm 2.0.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import scipy.sparse \n",
    "import lightgbm \n",
    "\n",
    "for p in [np, pd, scipy, sklearn, lightgbm]:\n",
    "    print (p.__name__, p.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this programming assignment you are asked to implement two ensembling schemes: simple linear mix and stacking.\n",
    "\n",
    "We will spend several cells to load data and create feature matrix, you can scroll down this part or try to understand what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "pd.set_option('display.max_rows', 600)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data from the hard drive first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('../readonly/final_project_data/sales_train.csv.gz')\n",
    "shops = pd.read_csv('../readonly/final_project_data/shops.csv')\n",
    "items = pd.read_csv('../readonly/final_project_data/items.csv')\n",
    "item_cats = pd.read_csv('../readonly/final_project_data/item_categories.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use only 3 shops for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sales[sales['shop_id'].isin([26, 27, 28])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to prepare the features. This part is all implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pandas/core/groupby.py:4036: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create \"grid\" with columns\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "\n",
    "# Turn the grid into a dataframe\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "\n",
    "# Groupby data to get shop-item-month aggregates\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n",
    "# Fix column names\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n",
    "# Join it to the grid\n",
    "all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Same as above but with shop-month aggregates\n",
    "gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Same as above but with item-month aggregates\n",
    "gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})\n",
    "gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Downcast dtypes from 64 to 32 bit to save memory\n",
    "all_data = downcast_dtypes(all_data)\n",
    "del grid, gb \n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a grid, we can calculate some features. We will use lags from [1, 2, 3, 4, 5, 12] months ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# List of columns that we will use to create lags\n",
    "cols_to_rename = list(all_data.columns.difference(index_cols)) \n",
    "\n",
    "shift_range = [1, 2, 3, 4, 5, 12]\n",
    "\n",
    "for month_shift in tqdm_notebook(shift_range):\n",
    "    train_shift = all_data[index_cols + cols_to_rename].copy()\n",
    "    \n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "    \n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n",
    "\n",
    "del train_shift\n",
    "\n",
    "# Don't use old data from year 2013\n",
    "all_data = all_data[all_data['date_block_num'] >= 12] \n",
    "\n",
    "# List of all lagged features\n",
    "fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n",
    "# We will drop these at fitting stage\n",
    "to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n",
    "\n",
    "# Category for each item\n",
    "item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n",
    "\n",
    "all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n",
    "all_data = downcast_dtypes(all_data)\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, we've created a feature matrix. It is stored in `all_data` variable. Take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>target</th>\n",
       "      <th>target_shop</th>\n",
       "      <th>target_item</th>\n",
       "      <th>target_lag_1</th>\n",
       "      <th>target_item_lag_1</th>\n",
       "      <th>target_shop_lag_1</th>\n",
       "      <th>target_lag_2</th>\n",
       "      <th>target_item_lag_2</th>\n",
       "      <th>target_shop_lag_2</th>\n",
       "      <th>target_lag_3</th>\n",
       "      <th>target_item_lag_3</th>\n",
       "      <th>target_shop_lag_3</th>\n",
       "      <th>target_lag_4</th>\n",
       "      <th>target_item_lag_4</th>\n",
       "      <th>target_shop_lag_4</th>\n",
       "      <th>target_lag_5</th>\n",
       "      <th>target_item_lag_5</th>\n",
       "      <th>target_shop_lag_5</th>\n",
       "      <th>target_lag_12</th>\n",
       "      <th>target_item_lag_12</th>\n",
       "      <th>target_shop_lag_12</th>\n",
       "      <th>item_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>10994</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6454.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>10992</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7521.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>10991</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5609.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6753.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7521.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>10988</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6454.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5609.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6753.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>11002</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id  item_id  date_block_num  target  target_shop  target_item  \\\n",
       "0       28    10994              12     1.0       6949.0          1.0   \n",
       "1       28    10992              12     3.0       6949.0          4.0   \n",
       "2       28    10991              12     1.0       6949.0          5.0   \n",
       "3       28    10988              12     1.0       6949.0          2.0   \n",
       "4       28    11002              12     1.0       6949.0          1.0   \n",
       "\n",
       "   target_lag_1  target_item_lag_1  target_shop_lag_1  target_lag_2  \\\n",
       "0           0.0                1.0             8499.0           0.0   \n",
       "1           3.0                7.0             8499.0           0.0   \n",
       "2           1.0                3.0             8499.0           0.0   \n",
       "3           2.0                5.0             8499.0           4.0   \n",
       "4           0.0                1.0             8499.0           0.0   \n",
       "\n",
       "   target_item_lag_2  target_shop_lag_2  target_lag_3  target_item_lag_3  \\\n",
       "0                1.0             6454.0           0.0                0.0   \n",
       "1                0.0                0.0           0.0                0.0   \n",
       "2                0.0                0.0           0.0                1.0   \n",
       "3                5.0             6454.0           5.0                6.0   \n",
       "4                0.0                0.0           0.0                0.0   \n",
       "\n",
       "   target_shop_lag_3  target_lag_4  target_item_lag_4  target_shop_lag_4  \\\n",
       "0                0.0           0.0                0.0                0.0   \n",
       "1                0.0           0.0                0.0                0.0   \n",
       "2             5609.0           0.0                2.0             6753.0   \n",
       "3             5609.0           0.0                2.0             6753.0   \n",
       "4                0.0           0.0                0.0                0.0   \n",
       "\n",
       "   target_lag_5  target_item_lag_5  target_shop_lag_5  target_lag_12  \\\n",
       "0           0.0                0.0                0.0            0.0   \n",
       "1           0.0                1.0             7521.0            0.0   \n",
       "2           2.0                4.0             7521.0            0.0   \n",
       "3           0.0                0.0                0.0            0.0   \n",
       "4           0.0                0.0                0.0            0.0   \n",
       "\n",
       "   target_item_lag_12  target_shop_lag_12  item_category_id  \n",
       "0                 0.0                 0.0                37  \n",
       "1                 0.0                 0.0                37  \n",
       "2                 0.0                 0.0                40  \n",
       "3                 0.0                 0.0                40  \n",
       "4                 0.0                 0.0                40  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>target</th>\n",
       "      <th>target_shop</th>\n",
       "      <th>target_item</th>\n",
       "      <th>target_lag_1</th>\n",
       "      <th>target_item_lag_1</th>\n",
       "      <th>target_shop_lag_1</th>\n",
       "      <th>target_lag_2</th>\n",
       "      <th>target_item_lag_2</th>\n",
       "      <th>target_shop_lag_2</th>\n",
       "      <th>target_lag_3</th>\n",
       "      <th>target_item_lag_3</th>\n",
       "      <th>target_shop_lag_3</th>\n",
       "      <th>target_lag_4</th>\n",
       "      <th>target_item_lag_4</th>\n",
       "      <th>target_shop_lag_4</th>\n",
       "      <th>target_lag_5</th>\n",
       "      <th>target_item_lag_5</th>\n",
       "      <th>target_shop_lag_5</th>\n",
       "      <th>target_lag_12</th>\n",
       "      <th>target_item_lag_12</th>\n",
       "      <th>target_shop_lag_12</th>\n",
       "      <th>item_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>157950.00000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "      <td>157950.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>27.00000</td>\n",
       "      <td>11175.577366</td>\n",
       "      <td>21.368262</td>\n",
       "      <td>1.423267</td>\n",
       "      <td>3639.332031</td>\n",
       "      <td>4.210510</td>\n",
       "      <td>1.293359</td>\n",
       "      <td>3.837354</td>\n",
       "      <td>2515.158203</td>\n",
       "      <td>1.267325</td>\n",
       "      <td>3.788079</td>\n",
       "      <td>2418.458496</td>\n",
       "      <td>1.226261</td>\n",
       "      <td>3.690402</td>\n",
       "      <td>2291.099121</td>\n",
       "      <td>1.199272</td>\n",
       "      <td>3.611922</td>\n",
       "      <td>2191.119385</td>\n",
       "      <td>1.173289</td>\n",
       "      <td>3.533542</td>\n",
       "      <td>2138.454346</td>\n",
       "      <td>0.834125</td>\n",
       "      <td>2.511706</td>\n",
       "      <td>1483.679443</td>\n",
       "      <td>43.666857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.82546</td>\n",
       "      <td>6245.324649</td>\n",
       "      <td>6.038875</td>\n",
       "      <td>7.808581</td>\n",
       "      <td>1773.740845</td>\n",
       "      <td>20.251411</td>\n",
       "      <td>8.239863</td>\n",
       "      <td>21.588100</td>\n",
       "      <td>2447.182129</td>\n",
       "      <td>8.384301</td>\n",
       "      <td>22.098978</td>\n",
       "      <td>2465.456299</td>\n",
       "      <td>8.470567</td>\n",
       "      <td>22.464106</td>\n",
       "      <td>2460.518066</td>\n",
       "      <td>8.695475</td>\n",
       "      <td>23.133760</td>\n",
       "      <td>2471.609619</td>\n",
       "      <td>8.845075</td>\n",
       "      <td>23.564177</td>\n",
       "      <td>2522.980957</td>\n",
       "      <td>7.616017</td>\n",
       "      <td>20.392336</td>\n",
       "      <td>2336.182861</td>\n",
       "      <td>16.246541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>26.00000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>26.00000</td>\n",
       "      <td>5601.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1910.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>27.00000</td>\n",
       "      <td>11207.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3784.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1910.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1764.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1587.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1486.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1339.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>28.00000</td>\n",
       "      <td>16249.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4675.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4282.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4403.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4282.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4282.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4297.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2597.000000</td>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.00000</td>\n",
       "      <td>22167.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>903.000000</td>\n",
       "      <td>7896.000000</td>\n",
       "      <td>1782.000000</td>\n",
       "      <td>903.000000</td>\n",
       "      <td>1782.000000</td>\n",
       "      <td>8499.000000</td>\n",
       "      <td>903.000000</td>\n",
       "      <td>1782.000000</td>\n",
       "      <td>8499.000000</td>\n",
       "      <td>903.000000</td>\n",
       "      <td>1782.000000</td>\n",
       "      <td>8499.000000</td>\n",
       "      <td>903.000000</td>\n",
       "      <td>1782.000000</td>\n",
       "      <td>8499.000000</td>\n",
       "      <td>903.000000</td>\n",
       "      <td>1782.000000</td>\n",
       "      <td>8499.000000</td>\n",
       "      <td>759.000000</td>\n",
       "      <td>1631.000000</td>\n",
       "      <td>8499.000000</td>\n",
       "      <td>83.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            shop_id        item_id  date_block_num         target  \\\n",
       "count  157950.00000  157950.000000   157950.000000  157950.000000   \n",
       "mean       27.00000   11175.577366       21.368262       1.423267   \n",
       "std         0.82546    6245.324649        6.038875       7.808581   \n",
       "min        26.00000      27.000000       12.000000      -2.000000   \n",
       "25%        26.00000    5601.000000       16.000000       0.000000   \n",
       "50%        27.00000   11207.000000       21.000000       1.000000   \n",
       "75%        28.00000   16249.000000       26.000000       1.000000   \n",
       "max        28.00000   22167.000000       33.000000     903.000000   \n",
       "\n",
       "         target_shop    target_item   target_lag_1  target_item_lag_1  \\\n",
       "count  157950.000000  157950.000000  157950.000000      157950.000000   \n",
       "mean     3639.332031       4.210510       1.293359           3.837354   \n",
       "std      1773.740845      20.251411       8.239863          21.588100   \n",
       "min        -1.000000      -2.000000      -1.000000          -1.000000   \n",
       "25%      1910.000000       1.000000       0.000000           0.000000   \n",
       "50%      3784.000000       2.000000       0.000000           1.000000   \n",
       "75%      4675.000000       3.000000       1.000000           3.000000   \n",
       "max      7896.000000    1782.000000     903.000000        1782.000000   \n",
       "\n",
       "       target_shop_lag_1   target_lag_2  target_item_lag_2  target_shop_lag_2  \\\n",
       "count      157950.000000  157950.000000      157950.000000      157950.000000   \n",
       "mean         2515.158203       1.267325           3.788079        2418.458496   \n",
       "std          2447.182129       8.384301          22.098978        2465.456299   \n",
       "min             0.000000      -2.000000          -1.000000           0.000000   \n",
       "25%             0.000000       0.000000           0.000000           0.000000   \n",
       "50%          1910.000000       0.000000           1.000000        1764.000000   \n",
       "75%          4282.000000       1.000000           3.000000        4403.000000   \n",
       "max          8499.000000     903.000000        1782.000000        8499.000000   \n",
       "\n",
       "        target_lag_3  target_item_lag_3  target_shop_lag_3   target_lag_4  \\\n",
       "count  157950.000000      157950.000000      157950.000000  157950.000000   \n",
       "mean        1.226261           3.690402        2291.099121       1.199272   \n",
       "std         8.470567          22.464106        2460.518066       8.695475   \n",
       "min        -1.000000          -1.000000           0.000000      -1.000000   \n",
       "25%         0.000000           0.000000           0.000000       0.000000   \n",
       "50%         0.000000           1.000000        1587.000000       0.000000   \n",
       "75%         1.000000           3.000000        4282.000000       1.000000   \n",
       "max       903.000000        1782.000000        8499.000000     903.000000   \n",
       "\n",
       "       target_item_lag_4  target_shop_lag_4   target_lag_5  target_item_lag_5  \\\n",
       "count      157950.000000      157950.000000  157950.000000      157950.000000   \n",
       "mean            3.611922        2191.119385       1.173289           3.533542   \n",
       "std            23.133760        2471.609619       8.845075          23.564177   \n",
       "min            -1.000000           0.000000      -1.000000          -1.000000   \n",
       "25%             0.000000           0.000000       0.000000           0.000000   \n",
       "50%             1.000000        1486.000000       0.000000           1.000000   \n",
       "75%             3.000000        4282.000000       1.000000           3.000000   \n",
       "max          1782.000000        8499.000000     903.000000        1782.000000   \n",
       "\n",
       "       target_shop_lag_5  target_lag_12  target_item_lag_12  \\\n",
       "count      157950.000000  157950.000000       157950.000000   \n",
       "mean         2138.454346       0.834125            2.511706   \n",
       "std          2522.980957       7.616017           20.392336   \n",
       "min             0.000000      -4.000000           -3.000000   \n",
       "25%             0.000000       0.000000            0.000000   \n",
       "50%          1339.000000       0.000000            0.000000   \n",
       "75%          4297.000000       0.000000            1.000000   \n",
       "max          8499.000000     759.000000         1631.000000   \n",
       "\n",
       "       target_shop_lag_12  item_category_id  \n",
       "count       157950.000000     157950.000000  \n",
       "mean          1483.679443         43.666857  \n",
       "std           2336.182861         16.246541  \n",
       "min              0.000000          2.000000  \n",
       "25%              0.000000         37.000000  \n",
       "50%              0.000000         40.000000  \n",
       "75%           2597.000000         55.000000  \n",
       "max           8499.000000         83.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sake of the programming assignment, let's artificially split the data into train and test. We will treat last month data as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test `date_block_num` is 33\n"
     ]
    }
   ],
   "source": [
    "# Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts \n",
    "dates = all_data['date_block_num']\n",
    "\n",
    "last_block = dates.max()\n",
    "print('Test `date_block_num` is %d' % last_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_train = dates[dates <  last_block]\n",
    "dates_test  = dates[dates == last_block]\n",
    "\n",
    "X_train = all_data.loc[dates <  last_block].drop(to_drop_cols, axis=1)\n",
    "X_test =  all_data.loc[dates == last_block].drop(to_drop_cols, axis=1)\n",
    "\n",
    "y_train = all_data.loc[dates <  last_block, 'target'].values\n",
    "y_test =  all_data.loc[dates == last_block, 'target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154596, 21)\n",
      "(3354, 21)\n",
      "(154596,)\n",
      "(3354,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. First level models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement a basic stacking scheme. We have a time component here, so we will use ***scheme f)*** from the reading material. Recall, that we always use first level models to build two datasets: test meta-features and 2-nd level train-metafetures. Let's see how we get test meta-features first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firts, we will run *linear regression* on numeric columns and get predictions for the last month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R-squared for linreg is 0.743180\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train.values, y_train)\n",
    "pred_lr = lr.predict(X_test.values)\n",
    "\n",
    "print('Test R-squared for linreg is %f' % r2_score(y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the we run *LightGBM*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R-squared for LightGBM is 0.738391\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "               'feature_fraction': 0.75,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':1, \n",
    "               'min_data_in_leaf': 2**7, \n",
    "               'bagging_fraction': 0.75, \n",
    "               'learning_rate': 0.03, \n",
    "               'objective': 'mse', \n",
    "               'bagging_seed': 2**7, \n",
    "               'num_leaves': 2**7,\n",
    "               'bagging_freq':1,\n",
    "               'verbose':0 \n",
    "              }\n",
    "\n",
    "model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)\n",
    "pred_lgb = model.predict(X_test)\n",
    "\n",
    "print('Test R-squared for LightGBM is %f' % r2_score(y_test, pred_lgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, concatenate test predictions to get test meta-features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_level2 = np.c_[pred_lr, pred_lgb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 13.45896153,  13.37831474],\n",
       "       [  3.18599444,   2.55590212],\n",
       "       [  2.5028209 ,   1.52356814],\n",
       "       [  1.54122935,   1.05787318],\n",
       "       [  1.13317243,   0.68982901],\n",
       "       [ 14.73598166,  12.81293152],\n",
       "       [ 34.31146362,  47.26912387],\n",
       "       [  0.88668341,   0.88510482],\n",
       "       [  1.88673223,   1.13225644],\n",
       "       [  3.84395862,   1.41531116]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_level2[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train meta-features\n",
    "\n",
    "**KFold scheme in time series**\n",
    "\n",
    "*In time-series task we usually have a fixed period of time we are asked to predict. Like day, week, month or arbitrary period with duration of T.*\n",
    "\n",
    "1. Split the train data into chunks of duration T. Select first M chunks.\n",
    "2. Fit N diverse models on those M chunks and predict for the chunk M+1. Then fit those models on first M+1 chunks and predict for chunk M+2 and so on, until you hit the end. After that use all train data to fit models and get predictions for test. Now we will have meta-features for the chunks starting from number M+1 as well as meta-features for the test.\n",
    "3. Now we can use meta-features from first K chunks [M+1,M+2,..,M+K] to fit level 2 models and validate them on chunk M+K+1. Essentially we are back to step 1. with the lesser amount of chunks and meta-features instead of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now it is your turn to write the code**. You need to implement ***scheme f)*** from the reading material. Here, we will use duration **T** equal to month and **M=15**.  \n",
    "\n",
    "That is, you need to get predictions (meta-features) from *linear regression* and *LightGBM* for months 27, 28, 29, 30, 31, 32. Use the same parameters as in above models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32])]\n",
    "\n",
    "# That is how we get target for the 2nd level dataset\n",
    "y_train_level2 = y_train[dates_train.isin([27, 28, 29, 30, 31, 32])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "[ 1.50148988  1.38811989]\n"
     ]
    }
   ],
   "source": [
    "# And here we create 2nd level feeture matrix, init it with zeros first\n",
    "X_train_level2 = np.zeros([y_train_level2.shape[0], 2])\n",
    "\n",
    "# Now fill `X_train_level2` with metafeatures\n",
    "for cur_block_num in [27, 28, 29, 30, 31, 32]:\n",
    "    \n",
    "    print(cur_block_num)\n",
    "    \n",
    "    '''\n",
    "        1. Split `X_train` into parts\n",
    "           Remember, that corresponding dates are stored in `dates_train`;\n",
    "        2. Fit linear regression ; \n",
    "        3. Fit LightGBM and put predictions;          \n",
    "        4. Store predictions from 2. and 3. in the right place of `X_train_level2`; \n",
    "           You can use `dates_train_level2` for it\n",
    "           Make sure the order of the meta-features is the same as in `X_test_level2`.\n",
    "    '''      \n",
    "    \n",
    "    # 1. Split `X_train` into parts\n",
    "    X = all_data.loc[dates <  cur_block_num].drop(to_drop_cols, axis=1).values\n",
    "    Y = all_data.loc[dates < cur_block_num, 'target'].values\n",
    "    X_test = all_data.loc[dates ==  cur_block_num].drop(to_drop_cols, axis=1)\n",
    "    \n",
    "    # 2. Fit linear regression\n",
    "    lr.fit(X, Y)\n",
    "    lr_pred = lr.predict(X_test)\n",
    "    \n",
    "    # 3. Fit LightGBM and put predictions\n",
    "    lgbm = lgb.train(lgb_params, lgb.Dataset(X, Y), num_boost_round=100)\n",
    "    lgbm_pred = lgbm.predict(X_test)\n",
    "    \n",
    "    # 4. Store predictions from 2. and 3. in the right place of `X_train_level2`\n",
    "    X_train_level2[dates_train_level2 == cur_block_num] = np.c_[lr_pred, lgbm_pred]\n",
    "    \n",
    "    \n",
    "# Sanity check\n",
    "print(X_train_level2.mean(axis=0))\n",
    "assert np.all(np.isclose(X_train_level2.mean(axis=0), [ 1.50148988,  1.38811989]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the ensembles work best, when first level models are diverse. We can qualitatively analyze the diversity by examinig *scatter plot* between the two metafeatures. Plot the scatter plot below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34404, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.19155666,  1.31854456],\n",
       "       [ 1.03517197,  0.99128266],\n",
       "       [ 1.55685619,  1.13375092],\n",
       "       [ 2.18989621,  1.58347887],\n",
       "       [ 2.28321904,  1.64823077],\n",
       "       [ 0.59141912,  0.70192199],\n",
       "       [ 0.66622456,  0.74784972],\n",
       "       [ 2.17527484,  0.89973551],\n",
       "       [ 0.03852802,  0.86362637],\n",
       "       [ 3.19863699,  1.53796047]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train_level2.shape)\n",
    "X_train_level2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f83ce6db358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.scatter(X_train_level2[:,0], X_train_level2[:,1], marker='o', edgecolor='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when the meta-features are created, we can ensemble our first level models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Simple convex mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with simple linear convex mix:\n",
    "\n",
    "$$\n",
    "mix= \\alpha\\cdot\\text{linreg_prediction}+(1-\\alpha)\\cdot\\text{lgb_prediction}\n",
    "$$\n",
    "\n",
    "We need to find an optimal $\\alpha$. And it is very easy, as it is feasible to do grid search. Next, find the optimal $\\alpha$ out of `alphas_to_try` array. Remember, that you need to use train meta-features (not test) when searching for $\\alpha$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.765000; Corresponding r2 score on train: 0.627255\n"
     ]
    }
   ],
   "source": [
    "alphas_to_try = np.linspace(0, 1, 1001)\n",
    "\n",
    "best_alpha = 0\n",
    "best_R2 = 0\n",
    "\n",
    "for alpha in alphas_to_try:\n",
    "    mix = alpha*X_train_level2[:,0] + (1-alpha)*X_train_level2[:,1]\n",
    "    temp_R2 = r2_score(y_train_level2, mix)\n",
    "    if temp_R2 > best_R2:\n",
    "        best_R2 = temp_R2\n",
    "        best_alpha = alpha\n",
    "\n",
    "print('Best alpha: %f; Corresponding r2 score on train: %f' % (best_alpha, best_R2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the $\\alpha$ you've found to compute predictions for the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R-squared for simple mix is 0.781144\n"
     ]
    }
   ],
   "source": [
    "test_preds = best_alpha*X_test_level2[:,0] + (1-best_alpha)*X_test_level2[:,1]\n",
    "r2_test_simple_mix = r2_score(y_test, test_preds)\n",
    "\n",
    "print('Test R-squared for simple mix is %f' % r2_test_simple_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try a more advanced ensembling technique. Fit a linear regression model to the meta-features. Use the same parameters as in the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_stack = LinearRegression().fit(X_train_level2, y_train_level2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute R-squared on the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-squared for stacking is 0.632176\n",
      "Test  R-squared for stacking is 0.771297\n"
     ]
    }
   ],
   "source": [
    "train_preds = lr_stack.predict(X_train_level2)\n",
    "r2_train_stacking = r2_score(y_train_level2, train_preds)\n",
    "\n",
    "test_preds = lr_stack.predict(X_test_level2)\n",
    "r2_test_stacking = r2_score(y_test, test_preds)\n",
    "\n",
    "print('Train R-squared for stacking is %f' % r2_train_stacking)\n",
    "print('Test  R-squared for stacking is %f' % r2_test_stacking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, that the score turned out to be lower than in previous method. Although the model is very simple (just 3 parameters) and, in fact, mixes predictions linearly, it looks like it managed to overfit. **Examine and compare** train and test scores for the two methods. \n",
    "\n",
    "And of course this particular case does not mean simple mix is always better than stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all done! Submit everything we need to the grader now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task best_alpha is: 0.765\n",
      "Current answer for task r2_train_simple_mix is: 0.627255043446\n",
      "Current answer for task r2_test_simple_mix is: 0.781144169579\n",
      "Current answer for task r2_train_stacking is: 0.632175561459\n",
      "Current answer for task r2_test_stacking is: 0.771297132342\n"
     ]
    }
   ],
   "source": [
    "from grader import Grader\n",
    "grader = Grader()\n",
    "\n",
    "grader.submit_tag('best_alpha', best_alpha)\n",
    "\n",
    "grader.submit_tag('r2_train_simple_mix', best_R2)\n",
    "grader.submit_tag('r2_test_simple_mix',  r2_test_simple_mix)\n",
    "\n",
    "grader.submit_tag('r2_train_stacking', r2_train_stacking)\n",
    "grader.submit_tag('r2_test_stacking',  r2_test_stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these numbers:\n",
      "Task best_alpha: 0.765\n",
      "Task r2_train_simple_mix: 0.627255043446\n",
      "Task r2_test_simple_mix: 0.781144169579\n",
      "Task r2_train_stacking: 0.632175561459\n",
      "Task r2_test_stacking: 0.771297132342\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EMAIL = '<EMAIL>'\n",
    "STUDENT_TOKEN = '<TOKEN>'\n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "6b650092311347e68fa646a0cca569ca": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
